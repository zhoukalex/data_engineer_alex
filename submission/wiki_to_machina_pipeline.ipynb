{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"dfd00175-5361-4d4a-912c-10e6bf3caa85","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Python interpreter will be restarted.\n","Requirement already satisfied: dbl-tempo in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e4ec9af6-d624-4e2d-8ac9-dda559df70f8/lib/python3.9/site-packages (0.1.22)\n","Requirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (from dbl-tempo) (1.3.4)\n","Requirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from dbl-tempo) (1.7.1)\n","Requirement already satisfied: ipython in /databricks/python3/lib/python3.9/site-packages (from dbl-tempo) (7.32.0)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (3.0.20)\n","Requirement already satisfied: traitlets>=4.2 in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (5.1.0)\n","Requirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (4.8.0)\n","Requirement already satisfied: pygments in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (2.10.0)\n","Requirement already satisfied: decorator in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (5.1.0)\n","Requirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (0.18.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython->dbl-tempo) (58.0.4)\n","Requirement already satisfied: backcall in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (0.2.0)\n","Requirement already satisfied: pickleshare in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (0.7.5)\n","Requirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (0.1.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /databricks/python3/lib/python3.9/site-packages (from jedi>=0.16->ipython->dbl-tempo) (0.8.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.9/site-packages (from pexpect>4.3->ipython->dbl-tempo) (0.7.0)\n","Requirement already satisfied: wcwidth in /databricks/python3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->dbl-tempo) (0.2.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->dbl-tempo) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->dbl-tempo) (2021.3)\n","Requirement already satisfied: numpy>=1.17.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->dbl-tempo) (1.20.3)\n","Requirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->dbl-tempo) (1.16.0)\n","Python interpreter will be restarted.\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Python interpreter will be restarted.\nRequirement already satisfied: dbl-tempo in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e4ec9af6-d624-4e2d-8ac9-dda559df70f8/lib/python3.9/site-packages (0.1.22)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (from dbl-tempo) (1.3.4)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from dbl-tempo) (1.7.1)\nRequirement already satisfied: ipython in /databricks/python3/lib/python3.9/site-packages (from dbl-tempo) (7.32.0)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (3.0.20)\nRequirement already satisfied: traitlets>=4.2 in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (5.1.0)\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (4.8.0)\nRequirement already satisfied: pygments in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (2.10.0)\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (5.1.0)\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (0.18.0)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython->dbl-tempo) (58.0.4)\nRequirement already satisfied: backcall in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (0.2.0)\nRequirement already satisfied: pickleshare in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (0.7.5)\nRequirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.9/site-packages (from ipython->dbl-tempo) (0.1.2)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /databricks/python3/lib/python3.9/site-packages (from jedi>=0.16->ipython->dbl-tempo) (0.8.2)\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.9/site-packages (from pexpect>4.3->ipython->dbl-tempo) (0.7.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->dbl-tempo) (0.2.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->dbl-tempo) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->dbl-tempo) (2021.3)\nRequirement already satisfied: numpy>=1.17.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->dbl-tempo) (1.20.3)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->dbl-tempo) (1.16.0)\nPython interpreter will be restarted.\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["%pip install dbl-tempo"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"40116980-6285-41f5-a8d7-1b5074df736d","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n","\u001b[0;32m<command-180995071469317>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n","\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001b[0m in \u001b[0;36mimport_patch\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n","\u001b[1;32m    169\u001b[0m             \u001b[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    170\u001b[0m             \u001b[0;31m# look at preceding stack frames for relevant error information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m--> 171\u001b[0;31m             \u001b[0moriginal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_builtin_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    173\u001b[0m             \u001b[0mis_root_import\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nest_level\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlt'"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n\u001b[0;32m<command-180995071469317>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001b[0m in \u001b[0;36mimport_patch\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# look at preceding stack frames for relevant error information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0moriginal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_builtin_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mis_root_import\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nest_level\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlt'","errorSummary":"<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'dlt'","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["import urllib\n","from pyspark.sql.window import Window\n","import pyspark.sql.functions as f\n","from pyspark.sql.functions import concat,col,concat_ws,to_timestamp\n","from pyspark.sql.types import *\n","from tempo import *\n","import boto3\n","from botocore.config import Config\n","# Note that the dlt library works while executing a pipeline workflow to build the table, \n","# it does not work when executing the commands within the python notebook itself.      \n","import dlt\n","\n","\n","@dlt.create_table(\n","  comment=\"The raw machina dataset, ingested from the github sample paruqet file\",\n","  table_properties={\n","    \"quality\": \"bronze\",\n","    \"pipelines.autoOptimize.managed\": \"true\"      \n","  }\n",")\n","def machina_raw():\n","    file_location = \"/FileStore/tables/new_user_credentials.csv\"\n","    file_type = \"csv\"\n","\n","    # unmount in case mounted\n","    dbutils.fs.unmount(\"/mnt/machina-stg-parquet\")    \n","    \n","    # Define file type\n","    file_type = \"csv\" # Whether the file has a header\n","    first_row_is_header = \"true\"# Delimiter used in the file\n","    delimiter = \",\"# Read the CSV file to spark dataframe\n","    aws_keys_df = spark.read.format(file_type)\\\n","    .option(\"header\", first_row_is_header)\\\n","    .option(\"sep\", delimiter)\\\n","    .load(file_location)\n","\n","    # Get the AWS access key and secret key from the spark dataframe\n","    ACCESS_KEY = aws_keys_df.where(col('User name')=='machina-log-user').select('Access key ID').collect()[0]['Access key ID']\n","    SECRET_KEY = aws_keys_df.where(col('User name')=='machina-log-user').select('Secret access key').collect()[0]['Secret access key']\n","\n","    # Encode the secrete key\n","    ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n","\n","    # AWS S3 bucket name\n","    # s3://machina-stg-parquet\n","    AWS_S3_BUCKET = \"machina-stg-parquet\"# Mount name for the bucket\n","    MOUNT_NAME = \"/mnt/machina-stg-parquet\"# Source url\n","    SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)# Mount the drive\n","    dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n","\n","    df = (spark.readStream.format(\"cloudFiles\") \\\n","          .option(\"cloudFiles.format\", \"parquet\") \\\n","          .option(\"mergeSchema\", \"true\") \\\n","          .schema(\"time STRING, value DOUBLE, field STRING, robot_id BIGINT, run_uuid DOUBLE, sensor_type STRING\") \\\n","          .load(\"/mnt/machina-stg-parquet\"))\n","    \n","    return (df) \n","\n","#################################################################################################################################################################################\n","#################################################################################################################################################################################\n","@dlt.create_table(\n","  comment=\"Machina raw data set processed for data quality issues.\",\n","  spark_conf={\"spark.databricks.delta.schema.autoMerge.enabled\": \"true\"},    \n","  table_properties={\n","    \"quality\": \"silver\", \n","    \"pipelines.autoOptimize.managed\": \"true\"      \n","  }, \n","    # https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/\n","    # https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/    \n","    # Pivot issue: \"I was able to get around this by specifying the table schema in the table decorator.\"\n","    # https://community.databricks.com/s/question/0D58Y00009AIhACSA1/delta-live-tables-not-inferring-table-schema-properly     \n","  schema=StructType([ \\\n","    StructField(\"run_uuid\",  DoubleType(), False), \\\n","    StructField(\"unix_milliseconds\",  DoubleType(), False), \\\n","    StructField(\"timestamp\", TimestampType(), False), \\\n","    StructField(\"fx_1\",  DoubleType(), True), \\\n","    StructField(\"fx_2\",  DoubleType(), True), \\\n","    StructField(\"fy_1\",  DoubleType(), True), \\\n","    StructField(\"fy_2\",  DoubleType(), True), \\\n","    StructField(\"fz_1\",  DoubleType(), True), \\\n","    StructField(\"fz_2\",  DoubleType(), True), \\\n","    StructField(\"x_1\",  DoubleType(), True), \\\n","    StructField(\"x_2\",  DoubleType(), True), \\\n","    StructField(\"y_1\",  DoubleType(), True), \\\n","    StructField(\"y_2\",  DoubleType(), True), \\\n","    StructField(\"z_1\",  DoubleType(), True), \\\n","    StructField(\"z_2\",  DoubleType(), True)\n","  ]),\n","  partition_cols=[\"run_uuid\"]  \n",")\n","@dlt.expect(\"valid_run_uuid_and_timestamp\", \"run_uuid IS NOT NULL AND timestamp IS NOT NULL\")\n","def machina_cleaned():\n","    df = dlt.read(\"machina_raw\") #\"clickstream_raw\")\n","    df = df.withColumn('field_robot_id', concat_ws('_',df.field,df.robot_id))\n","    df = df.withColumn(\"timestamp\", to_timestamp(col(\"time\")))\n","    df = df.drop(df['time'])  \n","    df = df.withColumn(\"unix_milliseconds\", col(\"timestamp\").cast(\"double\")*1000)\n","    df = df.dropDuplicates()\n","    df = df.orderBy(\"run_uuid\", \"unix_milliseconds\")    \n","    # The pivot() function is not supported. \n","    # The pivot operation in Spark requires eager loading of input data to compute the schema of the output. This capability is not supported in Delta Live Tables.\n","    # https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-python-ref.html    \n","    # https://community.databricks.com/s/question/0D58Y000098j9FrSAI/why-does-dlttable-from-a-table-give-different-results-than-from-a-view\n","    # pivot workaround. \n","    cols = [\"fx_1\", \"fx_2\", \"fy_1\", \"fy_2\", \"fz_1\", \"fz_2\", \"x_1\", \"x_2\", \"y_1\", \"y_2\", \"z_1\", \"z_2\"]\n","    pivotDF = df.groupBy(\"run_uuid\", \"unix_milliseconds\", \"timestamp\").pivot(\"field_robot_id\", cols).sum(\"value\")\n","    #pivotDF = pivotDF.withColumn(\"run_uuid\", pivotDF[\"run_uuid\"].cast(DecimalType(38, 0))) \n","    #pivotDF = pivotDF.withColumn(\"unix_milliseconds\", pivotDF[\"unix_milliseconds\"].cast(DecimalType(38, 0))) \n","    return pivotDF\n","\n","#################################################################################################################################################################################\n","#################################################################################################################################################################################\n","# Create another silver table, keep the cleaned up but non-processed data in machina_cleaned. \n","# In the new silver table, I can add new features, deal with null values, and resampple the time series. \n","@dlt.create_table(\n","  comment=\"Take transpose data in machina_cleaned, deal with null values, resample to 10 milliseconds, and add new feature columns.\",\n","  spark_conf={\"spark.databricks.delta.schema.autoMerge.enabled\": \"false\"},    \n","  table_properties={\n","    \"quality\": \"silver\", \n","    \"pipelines.autoOptimize.managed\": \"true\"      \n","  }, \n","    # https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/\n","    # https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/    \n","    # Pivot issue: \"I was able to get around this by specifying the table schema in the table decorator.\"\n","    # https://community.databricks.com/s/question/0D58Y00009AIhACSA1/delta-live-tables-not-inferring-table-schema-properly     \n","  schema=StructType([ \\\n","    StructField(\"run_uuid\",  DoubleType(), False), \\\n","    StructField(\"unix_milliseconds\",  DoubleType(), False), \\\n","    StructField(\"timestamp\", TimestampType(), False), \\\n","    StructField(\"fx_1\",  DoubleType(), True), \\\n","    StructField(\"fx_2\",  DoubleType(), True), \\\n","    StructField(\"fy_1\",  DoubleType(), True), \\\n","    StructField(\"fy_2\",  DoubleType(), True), \\\n","    StructField(\"fz_1\",  DoubleType(), True), \\\n","    StructField(\"fz_2\",  DoubleType(), True), \\\n","    StructField(\"x_1\",  DoubleType(), True), \\\n","    StructField(\"x_2\",  DoubleType(), True), \\\n","    StructField(\"y_1\",  DoubleType(), True), \\\n","    StructField(\"y_2\",  DoubleType(), True), \\\n","    StructField(\"z_1\",  DoubleType(), True), \\\n","    StructField(\"z_2\",  DoubleType(), True), \\\n","    StructField(\"vx_1\",  DoubleType(), True), \\\n","    StructField(\"vy_1\",  DoubleType(), True), \\\n","    StructField(\"vz_1\",  DoubleType(), True), \\\n","    StructField(\"vx_2\",  DoubleType(), True), \\\n","    StructField(\"vy_2\",  DoubleType(), True), \\\n","    StructField(\"vz_2\",  DoubleType(), True), \\\n","    StructField(\"ax_1\",  DoubleType(), True), \\\n","    StructField(\"ay_1\",  DoubleType(), True), \\\n","    StructField(\"az_1\",  DoubleType(), True), \\\n","    StructField(\"ax_2\",  DoubleType(), True), \\\n","    StructField(\"ay_2\",  DoubleType(), True), \\\n","    StructField(\"az_2\",  DoubleType(), True), \\\n","    StructField(\"v1\",  DoubleType(), True), \\\n","    StructField(\"v2\",  DoubleType(), True), \\\n","    StructField(\"a1\",  DoubleType(), True), \\\n","    StructField(\"a2\",  DoubleType(), True), \\\n","    StructField(\"f1\",  DoubleType(), True), \\\n","    StructField(\"f2\",  DoubleType(), True)\n","  ]),\n","  partition_cols=[\"run_uuid\"]  \n",")\n","@dlt.expect(\"valid_run_uuid_and_timestamp\", \"run_uuid IS NOT NULL AND timestamp IS NOT NULL\")\n","def machina_model_v1():\n","    df = dlt.read(\"machina_cleaned\") #\"clickstream_raw\") # machina_cleaned\n","    #df = df.withColumn(\"unix_milliseconds\", col(\"timestamp\").cast(\"double\")*1000)\n","    #df = df.orderBy(\"run_uuid\", \"unix_milliseconds\")    \n","\n","    df = df.drop(df['unix_milliseconds'])\n","    df_tsdf = TSDF(df, ts_col=\"timestamp\", partition_cols = [\"run_uuid\"])\n","    cols = [\"fx_1\", \"fx_2\", \"fy_1\", \"fy_2\", \"fz_1\", \"fz_2\", \"x_1\", \"x_2\", \"y_1\", \"y_2\", \"z_1\", \"z_2\"]\n","\n","    # What the following interpolation method does is:\n","    # 1. Aggregate columnA and columnBN  using mean into 10 ms intervals\n","    # 2. Interpolate any missing intervals or null values using linear fill and bfill for any remaining values. \n","    interpolated_tsdf = df_tsdf.interpolate(\n","        freq=\"10 ms\",\n","        func=\"mean\",\n","        target_cols= cols,\n","        method=\"ffill\" # ffill\n","    )\n","\n","    df2 = interpolated_tsdf.interpolate(\n","        freq=\"10 ms\",\n","        func=\"mean\",\n","        target_cols= cols,\n","        method=\"bfill\"\n","    )\n","\n","    #df_linear = df2.df \n","    #df_linear = df_linear.withColumn(\"unix_milliseconds\", col(\"timestamp\").cast(\"double\")*1000)    \n","    #df_linear = df_linear.orderBy(\"run_uuid\", \"timestamp\")      \n","    #df_linear = df_linear.select(\"run_uuid\", \"unix_milliseconds\", \"timestamp\", \"fx_1\", \"fx_2\", \"fy_1\", \"fy_2\", \"fz_1\", \"fz_2\", \"x_1\", \"x_2\", \"y_1\", \"y_2\", \"z_1\", \"z_2\")\n","\n","    df_ffill = df2.df \n","    df_ffill = df_ffill.withColumn(\"unix_milliseconds\", col(\"timestamp\").cast(\"double\")*1000)    \n","    df_ffill = df_ffill.orderBy(\"run_uuid\", \"timestamp\")      \n","    df_ffill = df_ffill.select(\"run_uuid\", \"unix_milliseconds\", \"timestamp\", \"fx_1\", \"fx_2\", \"fy_1\", \"fy_2\", \"fz_1\", \"fz_2\", \"x_1\", \"x_2\", \"y_1\", \"y_2\", \"z_1\", \"z_2\")   \n","\n","    window = Window.partitionBy(\"run_uuid\").orderBy(\"timestamp\")\n","    # .fillna(value=0) # This fills all null integer cols with 0\n","    df_ffill = df_ffill.withColumn(\"unix_milliseconds_delta\", f.col(\"unix_milliseconds\") - f.lag(f.col(\"unix_milliseconds\"), 1).over(window)).fillna(value=0) \n","    df_ffill = df_ffill.withColumn(\"x_1_delta\", f.col(\"x_1\") - f.lag(f.col(\"x_1\"), 1).over(window))\n","    df_ffill = df_ffill.withColumn(\"y_1_delta\", f.col(\"y_1\") - f.lag(f.col(\"y_1\"), 1).over(window))\n","    df_ffill = df_ffill.withColumn(\"z_1_delta\", f.col(\"z_1\") - f.lag(f.col(\"z_1\"), 1).over(window))\n","    df_ffill = df_ffill.withColumn(\"x_2_delta\", f.col(\"x_2\") - f.lag(f.col(\"x_2\"), 1).over(window))\n","    df_ffill = df_ffill.withColumn(\"y_2_delta\", f.col(\"y_2\") - f.lag(f.col(\"y_2\"), 1).over(window))\n","    df_ffill = df_ffill.withColumn(\"z_2_delta\", f.col(\"z_2\") - f.lag(f.col(\"z_2\"), 1).over(window)).fillna(value=0)\n","    df_ffill = df_ffill.withColumn(\"vx_1\", f.col(\"x_1_delta\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"vy_1\", f.col(\"y_1_delta\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"vz_1\", f.col(\"z_1_delta\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"vx_2\", f.col(\"x_2_delta\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"vy_2\", f.col(\"y_2_delta\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"vz_2\", f.col(\"z_2_delta\") / f.col(\"unix_milliseconds_delta\")).fillna(value=0)\n","    \n","    df_ffill = df_ffill.withColumn(\"ax_1\", f.col(\"vx_1\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"ay_1\", f.col(\"vy_1\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"az_1\", f.col(\"vz_1\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"ax_2\", f.col(\"vx_2\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"ay_2\", f.col(\"vy_2\") / f.col(\"unix_milliseconds_delta\"))\n","    df_ffill = df_ffill.withColumn(\"az_2\", f.col(\"vz_2\") / f.col(\"unix_milliseconds_delta\")).fillna(value=0)  \n","    # Placeholder columns until formula figured out. \n","    df_ffill = df_ffill.withColumn(\"v1\", f.lit(0.0))\n","    df_ffill = df_ffill.withColumn(\"v2\", f.lit(0.0))\n","    df_ffill = df_ffill.withColumn(\"a1\", f.lit(0.0))\n","    df_ffill = df_ffill.withColumn(\"a2\", f.lit(0.0))\n","    df_ffill = df_ffill.withColumn(\"f1\", f.lit(0.0))\n","    df_ffill = df_ffill.withColumn(\"f2\", f.lit(0.0))      \n","    \n","    cols_to_drop = (\"unix_milliseconds_delta\",\"x_1_delta\",\"y_1_delta\",\"z_1_delta\",\"x_2_delta\",\"y_2_delta\",\"z_2_delta\")\n","    df_ffill = df_ffill.drop(*cols_to_drop)    \n","    return df_ffill      \n","\n","#################################################################################################################################################################################\n","#################################################################################################################################################################################\n","@dlt.create_table(\n","  comment=\"Create stats per run id.\",\n","  spark_conf={\"spark.databricks.delta.schema.autoMerge.enabled\": \"true\"},    \n","  table_properties={\n","    \"quality\": \"gold\", \n","    \"pipelines.autoOptimize.managed\": \"true\"      \n","  },  \n","  schema=StructType([ \\\n","    StructField(\"run_uuid\",  DoubleType(), False), \\\n","    StructField(\"min_timestamp\", TimestampType(), True), \\\n","    StructField(\"max_timestamp\", TimestampType(), True), \\\n","    StructField(\"min_unix_milliseconds\",  DoubleType(), True), \\\n","    StructField(\"max_unix_milliseconds\",  DoubleType(), True), \\\n","    StructField(\"total_runtime_seconds\",  LongType(), True), \\\n","    StructField(\"total_runtime_ms\",  DoubleType(), True), \\\n","    StructField(\"total_distance_traveled\",  DoubleType(), True), \\\n","  ]),\n",")\n","@dlt.expect(\"valid_run_uuid\", \"run_uuid IS NOT NULL\")\n","def machina_run_uuid_stats():\n","    df = dlt.read(\"machina_model_v1\") #\"clickstream_raw\")\n","    window = Window.partitionBy(\"run_uuid\").orderBy(\"timestamp\")\n","\n","    groupDF = df.groupBy(\"run_uuid\").agg(f.min(\"timestamp\"), f.max(\"timestamp\"), f.min(\"unix_milliseconds\"), f.max(\"unix_milliseconds\"))\n","\n","    groupDF = groupDF.withColumn('total_runtime_seconds',col(\"max(timestamp)\").cast(\"long\") - col('min(timestamp)').cast(\"long\")) \\\n","        .withColumn('total_runtime_ms',col(\"max(unix_milliseconds)\") - col('min(unix_milliseconds)')) \\\n","        .withColumn('total_distance_traveled', f.lit(0.0)) \n","\n","    groupDF = groupDF.withColumnRenamed(\"min(timestamp)\", \"min_timestamp\")\n","    groupDF = groupDF.withColumnRenamed(\"max(timestamp)\", \"max_timestamp\")\n","    groupDF = groupDF.withColumnRenamed(\"min(unix_milliseconds)\", \"min_unix_milliseconds\")\n","    groupDF = groupDF.withColumnRenamed(\"max(unix_milliseconds)\", \"max_unix_milliseconds\")\n","    return groupDF"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"wiki_to_machina_pipeline","notebookOrigID":180995071469316,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
